# -*- coding: utf-8 -*-
"""catchment2Catchment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1udWf2SLLImR4MG3shX7xRf9T309Rc0wz
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from torchvision import transforms, utils, datasets
from tqdm import tqdm

class LinearNetwork(nn.Module):
  def __init__(self, in_dim = 365, out_dim = 365):
    super(LinearNetwork, self).__init__()
  
    
    self.net = nn.Sequential(nn.Linear(in_dim, 1000),
                            nn.LeakyReLU(),
                            nn.Linear(1000, 1000),
                            nn.LeakyReLU(),
                            nn.Linear(1000, 1000),
                            nn.LeakyReLU(),
                            nn.Linear(1000, 1000),
                            nn.LeakyReLU(),
                            nn.Linear(1000, 1000),
                            nn.LeakyReLU(),
                            nn.Linear(1000, 1000),
                            nn.LeakyReLU(),
                            nn.Linear(1000, out_dim))
    
  def forward(self, x):
    # n is the batch size, ch and w are used to calculate the dimensions in the flattened output vector
    return self.net(flattened)

class ResNetLayer(nn.Module):
                        
    def __init__(self, nChannels, tweakState):
        super(ResNetLayer, self).__init__()

        activationName = tweakState["activation"]

        self.batchnorm = tweakState["batchNorm"]

        self.nChannels = nChannels

        if tweakState["regularization"] == "off":
            dropoutRate = 0.0
        else:
            dropoutRate = 0.15
        self.dropout = nn.Dropout(dropoutRate)

        self.batchnormLayer = nn.BatchNorm1d(self.nChannels)

        self.conv1 = nn.Conv1d(in_channels=self.nChannels, out_channels=self.nChannels, kernel_size=3,padding=1)
        
        self.conv2 = nn.Conv1d(in_channels=self.nChannels, out_channels=self.nChannels, kernel_size=3,padding=1)

        if activationName == "relu": 
            self.activation = nn.ReLU()
        elif activationName == "leakyrelu":
            self.activation = nn.LeakyReLU()
        elif activationName == "selu":
            self.activation = nn.SELU()
        elif activationName == "hardshrink":
            self.activation = nn.Hardshrink()
        elif activationName == "elu":
            self.activation = nn.ELU()
        else:
            print(activationName)

    def forward(self, x):
        if self.batchnorm == "on":
            return x + self.batchnormLayer(self.conv1(self.dropout(self.activation(self.conv1(self.dropout(x))))))
        else:
            return x + self.conv1(self.dropout(self.activation(self.conv1(self.dropout(x)))))

class ResNet(nn.Module):
    def __init__(self, resParams, currentState):
        super(ResNet, self).__init__()

        self.currentState = currentState

        self.alpha = 0.1 
        self.resParams = resParams

        self.outChannels = resParams["outChannels"]

        # inChannels to resChannels
        self.inToRes = nn.Conv1d(in_channels= resParams["inChannels"], out_channels=resParams["outChannels"], kernel_size=3, padding=1)

        # resLayers
        self.resLayers = nn.ModuleList([])
        for layer in range(resParams["nLayers"]):
            self.resLayers.append(ResNetLayer(resParams["resChannels"], self.currentState))
        

        # outChannels to categories
        self.outToCategories = nn.Linear(in_features=365*3*resParams["resChannels"], out_features=resParams["categories"])

        initialization = self.currentState["initialization"]
        for p in self.parameters():
            if len(p.shape) > 1:
                if initialization=="xh":
                    nn.init.xavier_normal_(p)
                elif initialization=="orthogonal":
                    nn.init.orthogonal_(p)

    def forward(self, x):

        # inChannels to resChannels
        res = self.inToRes(x)

        for layer in self.resLayers:
            res = layer(res)

        # out = self.resToOut(res)
        out = res.view(-1, 365 * 3 * self.outChannels)
        categories = self.outToCategories(out)
        
        return categories

class TweakState():
    def __init__(self):
        self.possibleTweaks = {}
        self.possibleTweaks["activation"] = ["relu", "leakyrelu", "selu", "elu", "hardshrink"]
        self.possibleTweaks["batchNorm"] = ["off","on"]
        self.possibleTweaks["labelSmoothing"] = ["off","on"]
        self.possibleTweaks["learningRate"] = ["constant", "clr"]
        self.possibleTweaks["regularization"] = ["off", "dropout"]
        self.possibleTweaks["initialization"] = ["xh","orthogonal"]

        self.currentState = {}
        self.currentState["activation"] =       self.possibleTweaks["activation"][0]
        self.currentState["batchNorm"] =        self.possibleTweaks["batchNorm"][0]
        self.currentState["labelSmoothing"] =   self.possibleTweaks["labelSmoothing"][0]
        self.currentState["learningRate"] =     self.possibleTweaks["learningRate"][0]
        self.currentState["regularization"] =   self.possibleTweaks["regularization"][0]
        self.currentState["initialization"] =   self.possibleTweaks["initialization"][0]
 
        self.baselineState = {}
        self.baselineState["activation"] =       self.possibleTweaks["activation"][0]
        self.baselineState["batchNorm"] =        self.possibleTweaks["batchNorm"][0]
        self.baselineState["labelSmoothing"] =   self.possibleTweaks["labelSmoothing"][0]
        self.baselineState["learningRate"] =     self.possibleTweaks["learningRate"][0]
        self.baselineState["regularization"] =   self.possibleTweaks["regularization"][0]
        self.baselineState["initialization"] =   self.possibleTweaks["initialization"][0]
    
        self.resParams = {}
        self.resParams["nLayers"] = 50
        self.resParams["categories"] = 365
        self.resParams["inChannels"] = 1
        self.resParams["outChannels"] = 1
        self.resParams["resChannels"] = 1

        self.model = ResNet(self.resParams, self.baselineState)
        self.model = self.model.cuda()

    def updateState(self, tweakCategory,tweak):
        
        self.currentState[tweakCategory] = tweak
        # for category in self.possibleTweaks.keys():
        #     categoryTweaks = self.possibleTweaks[category]
        #     if tweak in categoryTweaks:
                # self.currentState[category] = tweak

        self.model = ResNet(self.resParams, self.currentState)
        self.model = self.model.cuda()


tweakState = TweakState()
tweakState.currentState["activation"] = "selu"
tweakState.currentState["batchNorm"] = "on"
tweakState.currentState["labelSmoothing"] = "off"
tweakState.currentState["learningRate"] = "clr"
tweakState.currentState["regularization"] = "dropout"
tweakState.currentState["initialization"] = "orthogonal"

model = tweakState.model
