import sys
import random
from tqdm import tqdm
import gc
import torch
import torch.nn as nn
import pickle
import os
import numpy as np
import matplotlib.pyplot as plt

class GruFlowtron(nn.Module):
    def __init__(self, numLayers):
        super(GruFlowtron, self).__init__()
        self.gru = nn.GRU(input_size=2938, hidden_size=1000, num_layers=numLayers, bias=True, batch_first=False, dropout=0,
                          bidirectional=False)
        self.gruToFlow = nn.Linear(in_features=1000, out_features=1)

    def forward(self, input, hiddens):
        o1, hiddensPrime = self.gru(input.view(1, 1, -1), hiddens)

        flow = self.gruToFlow(o1)

        return flow, hiddensPrime


# maybe in the future use multiple catchments

catchmentCharacteristicsFilePath = "normalized_zerod_catchment_characteristics.csv"
burnFilePath = "catchments_for_Q_analysis.csv"

numYearsToExclude = 10
startYear = 1984

numCatchmentsToCompare = 10000
numChunksPerComparison = 1000
numChunksPerValidation = 5
trainBurnIn = 10 
burnIn = 0 + trainBurnIn
chunkLength = 36 + burnIn

trainingDays = 10000 # 10227
valDays = trainingDays + (trainingDays // 10)
testDays = valDays + (trainingDays // 10)
flowCoefficient = 10000  # divide flow by this value to make the value less than 1 in most cases (and multiply the network output by this to get the real flow value!)
numLayers = 1
model = GruFlowtron(numLayers=numLayers).cuda()
model = model
modelSaveInterval = 50
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


class PercentageLoss(nn.Module):
    def __init__(self,):
        super(PercentageLoss, self).__init__()

    def forward(self, yHat, y):
        diff = torch.abs(yHat - y)
        percentDiff = diff / y
        return torch.sum(percentDiff) / y.shape[0]

objective = nn.L1Loss() # PercentageLoss() #  nn.L1Loss()


# ***************************************************** Functions and Classes


def getCatchmentCharacteristics(catchmentToGet):
    with open(catchmentCharacteristicsFilePath, "r+") as catchmentFile:
        for line in catchmentFile:
            characteristics = line.split(",")
            catchment = characteristics[0]
            if len(catchment) == 7:
                catchment = "0" + catchment
            if catchment == catchmentToGet:
                return characteristics
    return "NOT FOUND"

def getFlow(catchmentToGet, startIndex, stopIndex, burnChecker):
#    try:
        flowDir = "/home/sethbw/Documents/brian_flow_code/Data/all_flow"
        flowPath = os.path.join(flowDir, (catchmentToGet + ".npy"))
        flow = np.load(flowPath)
        flow.tolist()
        flow = burnChecker.removeBurnedData(flow, catchmentToGet)
        return flow[startIndex:stopIndex]

#    except:
#        print("exception occured while getting flow")
#        return []


class BurnChecker():
    def __init__(self, burnFilePath, numYearsToExclude, startYear):
        self.burnFilePath = burnFilePath
        self.numYearsToExclude = numYearsToExclude
        self.startYear = startYear

        self.catchmentToBurnedDays = {}
        self.initializeCatchmentToBurnedDays()

    def initializeCatchmentToBurnedDays(self):
        with open(self.burnFilePath) as burnFile:
            i = 0
            for line in burnFile:
                if i > 0:
                    data = line.split(",")
                    catchment = data[0]
                    startY = data[1]
                    startDate = startY + "-01-01"
                    endY = int(startY) + self.numYearsToExclude + 1
                    endDate = str(endY) + "-01-01"
                    startIndex = self.dateToIndex(startDate, self.startYear)
                    endIndex = self.dateToIndex(endDate, self.startYear)
                    if catchment in self.catchmentToBurnedDays.keys():
                        self.catchmentToBurnedDays[catchment].append((startIndex, endIndex))
                    else:
                        self.catchmentToBurnedDays[catchment] = []
                        self.catchmentToBurnedDays[catchment].append((startIndex, endIndex))
                i = i + 1

    def dateToIndex(self, date, startYear):
        year, month, day = date.split("-")
        year = int(year)
        month = int(month)
        day = int(day)

        index = 0

        # add the years *************************************************
        numYears = year - startYear
        for yearSinceStart in range(numYears):
            currentYear = yearSinceStart + startYear
            if currentYear % 4 == 0:
                daysInYear = 366
            else:
                daysInYear = 365

            index = index + daysInYear

        # add the months ***********************************************

        if year % 4 == 0:  # if it is a leap year
            monthToDays = {1: 31, 2: 29, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}
        else:
            monthToDays = {1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}

        monthVal = int(month)

        if monthVal > 1:
            for month in range(monthVal - 1):  # don't include the current month because it isn't over yet!
                month = month + 1  # start in January, not month 0
                index = index + monthToDays[month]

        # add the days *******************************************
        index = index + day

        return index

    def removeBurnedData(self, flow, catchmentToGet):
        if catchmentToGet in self.catchmentToBurnedDays.keys():
            burnedDays = self.catchmentToBurnedDays[catchmentToGet]

            for index in range(len(flow)):
                for startIndex, endIndex in burnedDays:
                    if index >= startIndex and index < endIndex:
                        flow[index] = ""
        return flow


class FlowChunker():

    def __init__(self, sourceCatchmentFlow, targetCatchmentFlow, chunkLength):
        self.chunkLength = chunkLength
        self.startIndicesToLoss = {}
        self.alpha = 20

        self.targetCatchmentFlow = targetCatchmentFlow
        self.sourceCatchmentFlow = sourceCatchmentFlow

        self.comparableIndices = []
        self.getComparableIndices()

    def clear_to_add(self):

        for key in self.startIndicesToLoss.keys():
            if self.startIndicesToLoss[key] > self.alpha:
                return False

        return True

    def getComparableIndices(self):
        
        # for every index, see if it is a valid index
        
        for i in range(len(self.sourceCatchmentFlow) - self.chunkLength - 20): # avoid the edge, where there is likely to be error
            comparableLength = 0
           
            # if the previous was already good, no need to verify the entire range -- just look one ahead
            if (i - 1) in self.comparableIndices:
                if self.sourceCatchmentFlow[i + chunkLength].isnumeric() and self.targetCatchmentFlow[i + chunkLength].isnumeric():
                    self.comparableIndices.append(i)
                           

            else:
                numContinuousValidDays = 0
                # examine every part of the chunk length to verify it is non-zero
                for j in range(chunkLength):
                    if self.sourceCatchmentFlow[i + j].isnumeric() and self.targetCatchmentFlow[i + j].isnumeric():
                        numContinuousValidDays += 1
 
                if numContinuousValidDays == chunkLength:
                    self.comparableIndices.append(i)

        # randomize the indices
        random.shuffle(self.comparableIndices)
        
        # add the first chunk
        newKey = self.comparableIndices[0]
        self.startIndicesToLoss[newKey] = int(1000)


    def add_new_random_num(self):
        
        if self.clear_to_add():
            newIndexNotAdded = False
            
            if len(self.startIndicesToLoss) < len(self.comparableIndices):
                newIndex = self.comparableIndices[len(self.startIndicesToLoss)]
                self.startIndicesToLoss[newIndex] = int(1000)

    def update_loss(self, startIndex, loss):
        self.startIndicesToLoss[startIndex] = loss

    def getChunk(self):

        self.add_new_random_num()
        keys = list(self.startIndicesToLoss.keys())
        randomStartIndex = random.randint(0, len(self.startIndicesToLoss.keys()) - 1)
        
        startIndex = keys[randomStartIndex]
        endIndex = startIndex + self.chunkLength

        targetFlowSelection = self.targetCatchmentFlow[startIndex:endIndex]
        sourceFlowSelection = self.sourceCatchmentFlow[startIndex:endIndex]
        
        return targetFlowSelection, sourceFlowSelection, startIndex


class CatchmentPicker():
    def __init__(self, catchments, burnFilePath, numYearsToExclude, startYear):
        self.catchments = catchments

        self.burnChecker = BurnChecker(burnFilePath, numYearsToExclude, startYear)

    def flowsComparable(self, sourceFlow, targetFlow):
        if len(sourceFlow) != len(targetFlow):
            return False

        comparableLength = 0
        for i in range(len(sourceFlow)):

            if sourceFlow[i].isnumeric() and targetFlow[i].isnumeric():
                comparableLength += 1
            else:
                comparableLength = 0

            if comparableLength > chunkLength:
                return True

        return False

    def catchmentsComparable(self, sourceCatchment, targetCatchment):

        sourceTestFlow = getFlow(sourceCatchment, 0, trainingDays, self.burnChecker)
        sourceValFlow = getFlow(sourceCatchment, trainingDays, valDays, self.burnChecker)
        sourceTrainFlow = getFlow(sourceCatchment, valDays, -1, self.burnChecker)

        targetTestFlow = getFlow(targetCatchment, 0, trainingDays, self.burnChecker)
        targetValFlow = getFlow(targetCatchment, trainingDays, valDays, self.burnChecker)
        targetTrainFlow = getFlow(targetCatchment, valDays, -1, self.burnChecker)

        if not self.flowsComparable(sourceTestFlow, targetTestFlow):
            return False
        if not self.flowsComparable(sourceValFlow, targetValFlow):
            return False
        if not self.flowsComparable(sourceTrainFlow, targetTrainFlow):
            return False

        return True


    def pickTwoCatchments(self, comparisonNum):

        catchmentsNotComparable = True

        while catchmentsNotComparable:

            sourceIndex = random.randint(0, (len(self.catchments) // 10))

            # expand number of catchments considered gradually (this helps stabilize the network)
            if comparisonNum > 30:
                 sourceIndex = random.randint(0, (len(self.catchments) // 10) * 2)
            elif comparisonNum > 60:
                 sourceIndex = random.randint(0, (len(self.catchments) // 10) * 3)
            elif comparisonNum > 90:
                 sourceIndex = random.randint(0, (len(self.catchments) // 10) * 4)
            elif comparisonNum > 120:
                 sourceIndex = random.randint(0, (len(self.catchments) // 10) * 5)
            elif comparisonNum > 150:
                 sourceIndex = random.randint(0, (len(self.catchments) // 10) * 6)
            elif comparisonNum > 180:
                 sourceIndex = random.randint(0, (len(self.catchments) // 10) * 7)
            elif comparisonNum > 210:
                 sourceIndex = random.randint(0, (len(self.catchments) // 10) * 8)
            elif comparisonNum > 250:
                 sourceIndex = random.randint(0, (len(self.catchments) // 10) * 9)
            elif comparisonNum > 280:
                sourceIndex = random.randint(0, len(self.catchments) - 1)

            targetIndex = len(self.catchments) + 1
            randomNum = 0
            while randomNum == 0 or (targetIndex > (len(self.catchments) - 1)):
                randomNum = random.randint(-5, 5)

                targetIndex = sourceIndex + randomNum
            sourceCatchment = self.catchments[sourceIndex]
            targetCatchment = self.catchments[targetIndex]

            if self.catchmentsComparable(sourceCatchment, targetCatchment):
                catchmentsNotComparable = False
        
        return sourceCatchment, targetCatchment


def initializeHiddens(sourceCatchmentChars, targetCatchmentChars, numLayers):
    try:
        zerosSource = torch.ones(32)
        source = [float(i) for i in sourceCatchmentChars[1:]]
        source = torch.FloatTensor(source)
    except:
        print('source')
        print(sourceCatchmentChars)
        return -1

    try:
        zerosTarget = torch.ones(32)
        target = [float(i) for i in targetCatchmentChars[1:]]
        target = torch.FloatTensor(target)
    except:
        print('target')
        print(targetCatchmentChars)
        return -1
        pass

    hidden = torch.cat((zerosSource, source, zerosTarget, target), dim=0)
    
#    print(hidden.shape)
    # for layer in range(numLayers - 1):
    # hidden = torch.cat((hidden, torch.ones(1000)), dim=0)
    if numLayers > 1:
        newHiddens = []
        newHiddens.append(torch.ones(1000))
#        newHiddens.append(hidden)
        for layer in range(numLayers - 1):
#            newHiddens.append(hidden)
            newHiddens.append(torch.ones(1000))
            # hidden = torch.cat((hidden, torch.ones(1000)), dim=1)

        hidden = torch.stack(newHiddens)
        hidden = hidden.unsqueeze(dim=1)
    else:
        hidden = hidden.view(1,1,-1)
        # print(hidden.shape)
    # hidden = hidden

    return hidden.cuda()



def evaluateModel(model, valChunker, sourceCharacteristics, targetCharacteristics, objective):
    gc.collect()
    model.eval()
    losses = []
    for _ in range(numChunksPerValidation):

        totalLoss = 0

        # get a chunk
        sourceFlowChunk, targetFlowChunk, startIndex = valChunker.getChunk()

        predictedFlow = []
        # initiate the hiddens
        hiddens = initializeHiddens(sourceCharacteristics, targetCharacteristics, numLayers)
        if type(hiddens) == type(-1):
            print('eval')
            print(sourceCharacteristics)
            print(targetCharacteristics)

        fSourceCharacteristics = [float(num) for num in sourceCharacteristics]
        fTargetCharacteristics = [float(num) for num in targetCharacteristics]

        sourceChars = torch.FloatTensor(fSourceCharacteristics)
        targetChars = torch.FloatTensor(fTargetCharacteristics)
        catchmentInfo = torch.cat((sourceChars, targetChars), dim=0).cuda()

        # for char in chunk:
        output = targetFlowChunk[0]
        predictedFlow.append(output)
        for j in range(1, len(sourceFlowChunk)):
            input = makeFlowTensor(sourceFlowChunk[j], output)
            input = torch.cat((input, catchmentInfo), dim=0)

            output, hiddens = model(input, hiddens)
            output = output.squeeze(dim=0).squeeze(dim=0)
            predictedFlow.append(output.item() * flowCoefficient)
            correctOutput = torch.FloatTensor([float(targetFlowChunk[j]) / flowCoefficient]).cuda()
            loss = objective(output, correctOutput)
            if j > trainBurnIn:
                totalLoss = totalLoss + loss

        losses.append(totalLoss)

    # xs = range(chunkLength + 1)
    # print(sourceFlowChunk)
    # print(targetFlowChunk)

    # print(len(xs))
    # print(len(sourceFlowChunk))
    # print(len(targetFlowChunk)) 
    # print(len(predictedFlow))

    sourceFlow = [float(f) for f in sourceFlowChunk]
    targetFlow = [float(f) for f in targetFlowChunk]
    predictedFlow = [float(f) for f in predictedFlow]

    np.save("sourceFlow",sourceFlow)
    np.save("targetFlow",targetFlow)
    np.save("preditedFlow", predictedFlow)

#    plt.plot(sourceFlow, label="sourceFlow")
#    plt.plot(targetFlow, label="targetFlow")
#    plt.plot(predictedFlow, label="predictedFlow")
#    plt.legend()
#    plt.show()

    model.train()
    gc.collect()
    return torch.mean(torch.FloatTensor(losses))


def makeFlowTensor(sourceString, targetString):
    try:
        sourceFloat = float(sourceString) / flowCoefficient
    except:
        print('exception in makeFlowTensor:')
        sourceFloat = 0.0
    try:
        targetFloat = float(targetString) / flowCoefficient
    except:
        print('exception in makeFlowTensor:')
        targetFloat = 0.0

    flowList = [sourceFloat, targetFloat] * 1000
    flowTensor = torch.FloatTensor(flowList)

    return flowTensor.cuda()


# ************************************************ Action code starts here

# get a list of catchments present in flowFile:
catchments = []
with open(catchmentCharacteristicsFilePath, "r+") as flowFile:
    i = 0
    for line in flowFile:
        if i > 0:
            lineList = line.split(",")
            catchment = lineList[0]
            catchments.append(catchment)
        i = i + 1


picker = CatchmentPicker(catchments, burnFilePath, numYearsToExclude, startYear)

# train!

trainingLosses = []
validationLosses = []

for i in range(numCatchmentsToCompare):

    # cool down the learning rate gradually (the more variability in the data, the lower the lr needs to be)
    if i == 100:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)
    if i == 200:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)
    if i == 300:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)
    if i == 400:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)
    if i == 500:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.000005)
    if i == 600:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)
    if i == 700:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.0000005)
    if i == 1000:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.0000001)


#    if i > 20 and i < 50:
#        numChunksPerComparison += 2

    sourceCatchment, targetCatchment = picker.pickTwoCatchments(i)

    sourceTrainFlow = getFlow(sourceCatchment, 0, trainingDays, picker.burnChecker)
    targetTrainFlow = getFlow(targetCatchment, 0, trainingDays, picker.burnChecker)

    sourceValFlow = getFlow(sourceCatchment, trainingDays, valDays, picker.burnChecker)
    targetValFlow = getFlow(targetCatchment, trainingDays, valDays, picker.burnChecker)

    trainChunker = FlowChunker(sourceTrainFlow, targetTrainFlow, chunkLength)
    valChunker = FlowChunker(sourceValFlow, targetValFlow, chunkLength)

    sourceCharacteristics = getCatchmentCharacteristics(sourceCatchment)
    targetCharacteristics = getCatchmentCharacteristics(targetCatchment)


    fSourceCharacteristics = [float(num) for num in sourceCharacteristics]
    fTargetCharacteristics = [float(num) for num in targetCharacteristics]

    sourceChars = torch.FloatTensor(fSourceCharacteristics)
    targetChars = torch.FloatTensor(fTargetCharacteristics)
    catchmentInfo = torch.cat((sourceChars, targetChars), dim=0).cuda()

    print()
    print()
    print("source catchment: " + str(sourceCatchment))
    print("target catchment: " + str(targetCatchment))
    print()

    loop = tqdm(total=numChunksPerComparison, position=0, leave=False)

    for chunk in range(numChunksPerComparison):

        optimizer.zero_grad()
        totalLoss = 0

        # get a chunk
        sourceFlowChunk, targetFlowChunk, startIndex = trainChunker.getChunk()

        # initiate the hiddens
        hiddens = initializeHiddens(sourceCharacteristics, targetCharacteristics, numLayers)
        if type(hiddens) == type(-1):
            print('train')
            print(sourceCatchment)
            print(sourceCharacteristics)
            print(targetCatchment)
            print(targetCharacteristics)
        # for char in chunk:
        output = targetFlowChunk[0]
        predictedFlow = []
        predictedFlow.append(output)
        for j in range(1, len(sourceFlowChunk) - 1):
            # get input (the source flow + (target - 1))
            if i > 0:
                input = makeFlowTensor(sourceFlowChunk[j], output)
                input = torch.cat((input,catchmentInfo), dim=0)
            else:
                input = makeFlowTensor(sourceFlowChunk[j], targetFlowChunk[j - 1])
                input = torch.cat((input,catchmentInfo), dim=0)


            output, hiddens = model(input, hiddens)
            output = output.squeeze(dim=0).squeeze(dim=0)
            predictedFlow.append(output * flowCoefficient)
            correctOutput = torch.FloatTensor([float(targetFlowChunk[j]) / flowCoefficient]).cuda()
            loss = objective(output, correctOutput)

            if j > trainBurnIn:
                totalLoss = totalLoss + loss

#        sourceFlow = [float(f) for f in sourceFlowChunk[burnIn:]]
#        targetFlow = [float(f) for f in targetFlowChunk[burnIn:]]
#        predictedFlow = [float(f) for f in predictedFlow[burnIn:]]

#        predictedFlowTensor = torch.FloatTensor(predictedFlow)

        # clamp the loss at 40
#        if i > 40:
#            totalLoss = torch.clamp(totalLoss, min=0, max=30)
#        if i > 60:
#            totalLoss = torch.clamp(totalLoss, min=0, max=20)
#        if i > 80:
#            totalLoss = torch.clamp(totalLoss, min=0, max=15)

        # std = predictedFlowTensor.std(dim=0).item()
        # if std < 1000:
        #     print()
        #     print()
        #     lossBonus = (1 / std) * 10000
        #     print('loss bonus')
        #     print(lossBonus)
        #     print()
        #     totalLoss += lossBonus

        # sns.lineplot(y=sourceFlow, x=xs)
        # plt.plot(sourceFlow, label="sourceFlow")
        # plt.plot(targetFlow, label="targetFlow")
        # plt.plot(predictedFlow, label="predictedFlow")
        # plt.legend()
        # plt.show()

        if chunk % 5 == 0:
            valLoss = evaluateModel(model, valChunker, sourceCharacteristics, targetCharacteristics, objective)
            validationLosses.append((len(trainingLosses), valLoss))

        trainChunker.update_loss(startIndex, totalLoss)

        loop.set_description(
            'comparison_num: {}, trainLoss: {:,.4f}, valLoss: {:,.4f}, num_chunks: {}'.format(i, totalLoss, valLoss,
                                                                                              len(
                                                                                                  list(
                                                                                                      trainChunker.startIndicesToLoss.keys()))))

        loop.update(1)
        totalLoss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)

        optimizer.step()

        trainingLosses.append(totalLoss.item())

    if i % modelSaveInterval == 0:
        torch.save(model.state_dict(), 'model-' + str(i))
        pickle.dump(trainingLosses, open(("trainingLosses-" + str(i)), "wb"))
        pickle.dump(validationLosses, open(("validationLosses-" + str(i)), "wb"))
# extract the flow data for each
# remove flow data that is burned or that is reserved for validation and testing ** save the last 20% of valid comparison days (remove anything after Jan 1 2012
#

